# CrewAI with LM Studio Integration

A CrewAI project configured to work seamlessly with LM Studio for local LLM inference. Features a simple CLI for switching between models and comprehensive tests for connectivity verification.

## Key Features

- **LM Studio Integration**: Direct connectivity to local LM Studio server
- **Model Switching CLI**: Number-based interface for quick model changes
- **Connectivity Tests**: Verify LM Studio connection and CrewAI integration
- **Configuration Management**: Simple .env.toml for model settings

## Quick Start

1. **Setup LM Studio**: Ensure LM Studio is running on `localhost:1234` with models loaded
2. **Install dependencies**: `uv sync` (installs from `pyproject.toml`, creates `uv.lock`)
3. **Configure models**: `cp .env.toml.example .env.toml` then auto-sync or manually edit
4. **Auto-sync models**: `uv run python scripts/sync_models.py sync` (optional - auto-detects LM Studio models)
5. **Test connectivity**: `uv run python tests/test_lm_studio_simple.py`
6. **Switch models**: `uv run python scripts/switch_model.py 1`
7. **Run CrewAI**: `uv run hello_crewai`

## Git Workflow

**What you get from GitHub:**
- `pyproject.toml` - Dependencies and project configuration
- `.env.toml.example` - Configuration template
- Source code, scripts, tests, and documentation

**What you create locally (git-ignored):**
- `.env.toml` - Your personal model configuration
- `uv.lock` - Dependency lock file (auto-generated by `uv sync`)
- `.venv/` - Virtual environment

## Model Switching CLI

The project includes a number-based CLI for easy model switching:

```bash
# List available models with numbers
uv run python scripts/switch_model.py list

# Switch to model by number (1, 2, etc.)
uv run python scripts/switch_model.py 1
uv run python scripts/switch_model.py 2
```

## Auto-Sync Models from LM Studio

Automatically detect and sync models from your running LM Studio instance:

```bash
# List models currently loaded in LM Studio
uv run python scripts/sync_models.py list

# Sync LM Studio models to your configuration
uv run python scripts/sync_models.py sync
```

This eliminates the need to manually edit `.env.toml` when you add new models to LM Studio!

## Adding New Models

When you add new models to LM Studio, you have two options to make them available in CrewAI:

### Method 1: Auto-Sync (Easy)

```bash
# After loading new models in LM Studio, just run:
uv run python scripts/sync_models.py sync
```

### Method 2: Manual Addition

1. Load your model in LM Studio
2. Note the exact model name as shown in LM Studio's interface
3. Add to `.env.toml`:

    ```toml
    [models.my-new-model]
    name = "exact-model-name-from-lm-studio"
    timeout = 300
    description = "My new model description"
    ```

**Pro Tip:** Use `uv run python scripts/sync_models.py list` to see the exact model names as they appear in LM Studio's API.

## Testing

Verify your setup with the included tests:

```bash
# Test LM Studio connectivity
uv run python tests/test_lm_studio_simple.py

# Test CrewAI integration
uv run python tests/test_crewai_simple.py
```

## Project Structure

```
hello_crewai/
├── .env.toml.example        # Configuration template
├── scripts/
│   ├── switch_model.py      # Model switching CLI
│   └── sync_models.py       # Auto-sync models from LM Studio
├── tests/                   # LM Studio & CrewAI tests
├── src/hello_crewai/        # Main application
└── pyproject.toml          # Dependencies
```

## Configuration

The project uses `.env.toml` for model configuration. You have two options:

### Option 1: Automatic Sync (Recommended)
Automatically detect and sync all models from LM Studio:

```bash
# Copy template and sync models
cp .env.toml.example .env.toml
uv run python scripts/sync_models.py sync
```

### Option 2: Manual Configuration
Add models manually to `.env.toml`:

```toml
[models.your-model-key]
name = "exact-model-name-from-lm-studio"
timeout = 300
description = "Your model description"
```

**Example:** If you load `llama-3.1-8b-instruct` in LM Studio:

```toml
[models.llama-8b]
name = "llama-3.1-8b-instruct"
timeout = 300
description = "Llama 3.1 8B - Fast and versatile"
```

**Note:** Model names must match exactly as they appear in LM Studio (without the `openai/` prefix that appears in API calls).

## Requirements

- Python >=3.10 <3.14
- [UV](https://docs.astral.sh/uv/) package manager
- LM Studio running on `localhost:1234`

## Troubleshooting

### Dependency Installation Issues
If `uv sync` fails, try these alternatives:

```bash
# Option 1: Use pip as fallback
pip install -e .

# Option 2: Install UV first
pip install uv
uv sync

# Option 3: Use traditional pip with requirements
pip install crewai python-dotenv toml requests
```

### Model Sync Issues
If auto-sync doesn't work:

```bash
# Check if LM Studio is running
curl http://localhost:1234/v1/models

# Manually add models to .env.toml (see Configuration section)
```

## Support

For support, questions, or feedback:
- Visit the [CrewAI documentation](https://docs.crewai.com)
- Check the [CrewAI GitHub repository](https://github.com/joaomdmoura/crewai)
- Join the [CrewAI Discord](https://discord.com/invite/X4JWnZnxPb)

Let's create wonders together with CrewAI and local LLMs!
